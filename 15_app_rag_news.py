# pip install streamlit rank_bm25 faiss-cpu sentence-transformers transformers accelerate

# 1) ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ------------------------------------------------------------
import os, json, sqlite3, torch
import streamlit as st
from langchain_community.retrievers import BM25Retriever                 # í‚¤ì›Œë“œ ê¸°ë°˜
from langchain_community.vectorstores import FAISS                       # ë²¡í„° ì €ì¥/ê²€ìƒ‰
from langchain_huggingface import HuggingFaceEmbeddings                  # <- ê¶Œì¥ ì„ë² ë”©
from langchain.retrievers import EnsembleRetriever                       # ì•™ìƒë¸”
import transformers                                                      # ë¡œì»¬ LLM íŒŒì´í”„ë¼ì¸

# 2) DB ê²½ë¡œ/í…Œì´ë¸” ì„¤ì • ----------------------------------------------------------
DB_PATH = "company_news.db"
TABLE   = "news"

# 3) DBì—ì„œ ë¬¸ì„œ ë¡œë“œ í•¨ìˆ˜ --------------------------------------------------------
def load_documents_from_sqlite(db_path: str):
    """
    - ë°˜í™˜: texts(list[str]), metadatas(list[dict])
    - texts: ìš”ì•½ ì»¬ëŸ¼ì„ ì‚¬ìš©í•˜ì—¬ ì„ë² ë”©/ê²€ìƒ‰ì— í™œìš©
    - metadatas: ê¸°ì—…ëª…/ë‚ ì§œ/ì¹´í…Œê³ ë¦¬/ì´ë²¤íŠ¸ ë“± ë¶€ê°€ì •ë³´
    """
    if not os.path.exists(db_path):
        raise FileNotFoundError(f"{db_path} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë°ì´í„° ìƒì„± ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")

    conn = sqlite3.connect(db_path)
    cur  = conn.cursor()
    cur.execute(f"SELECT id, ê¸°ì—…ëª…, ë‚ ì§œ, ë¬¸ì„œ_ì¹´í…Œê³ ë¦¬, ìš”ì•½, ì£¼ìš”_ì´ë²¤íŠ¸ FROM {TABLE} ORDER BY ë‚ ì§œ ASC")
    rows = cur.fetchall()
    conn.close()

    texts, metadatas = [], []
    for rid, company, date, category, summary, events_json in rows:
        texts.append(summary)
        try:
            events = ", ".join(json.loads(events_json))
        except Exception:
            events = events_json
        metadatas.append({
            "id": rid,
            "ê¸°ì—…ëª…": company,
            "ë‚ ì§œ": date,
            "ë¬¸ì„œ_ì¹´í…Œê³ ë¦¬": category,
            "ì£¼ìš”_ì´ë²¤íŠ¸": events,
            "source": f"db_doc_{rid}",
        })
    return texts, metadatas

# 4) ì•™ìƒë¸” Retriever êµ¬ì„±(BM25 + FAISS) ----------------------------------------
def build_ensemble_retriever(texts, metadatas):
    # 4-1) BM25: í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ê¸° (k=2)
    bm25 = BM25Retriever.from_texts(texts, metadatas=metadatas)
    bm25.k = 2

    # 4-2) ì„ë² ë”©(ë¡œì»¬) + FAISS: ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ê¸° (k=2)
    #      - í•œêµ­ì–´/ë‹¤êµ­ì–´ì— ë¬´ë‚œí•œ ë¬¸ì¥ ì„ë² ë”© ëª¨ë¸ ì˜ˆì‹œ
    embedding = HuggingFaceEmbeddings(model_name="distiluse-base-multilingual-cased-v1")
    faiss_store = FAISS.from_texts(texts, embedding, metadatas=metadatas)
    faiss = faiss_store.as_retriever(search_kwargs={"k": 2})

    # 4-3) ì•™ìƒë¸”: BM25(0.3) + FAISS(0.7) ê°€ì¤‘í•©
    ensemble = EnsembleRetriever(retrievers=[bm25, faiss], weights=[0.3, 0.7])
    return ensemble

# 5) ë¡œì»¬ LLM íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”(ì˜ˆ: 42dot/42dot_LLM-SFT-1.3B) -------------------
@st.cache_resource(show_spinner=False)
def load_local_pipeline(model_id: str = "42dot/42dot_LLM-SFT-1.3B"):
    """
    - GPU ìˆìœ¼ë©´ float16ë¡œ, ì—†ìœ¼ë©´ float32ë¡œ ìë™ ì„ íƒ
    - ì¶”ë¡ (í‰ê°€) ëª¨ë“œë¡œ ì„¤ì •
    """
    use_cuda   = torch.cuda.is_available()
    torch_dtype = torch.float16 if use_cuda else torch.float32

    pipe = transformers.pipeline(
        task="text-generation",
        model=model_id,
        model_kwargs={"torch_dtype": torch_dtype},
        device_map="auto" if use_cuda else None,
    )
    pipe.model.eval()
    return pipe

# 6) ê²€ìƒ‰ í•¨ìˆ˜(ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜) --------------------------------------------------
def search(query: str, retriever):
    docs = retriever.invoke(query)
    return docs or []

# 7) RAG í”„ë¡¬í”„íŠ¸ êµ¬ì„±(ë¬¸ì„œê°€ 'ìˆì„ ë•Œë§Œ' í˜¸ì¶œ) ----------------------------------
def build_prompt(query: str, docs):
    """
    - ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ 'ìë£Œ' ì„¹ì…˜ìœ¼ë¡œ ë‚˜ì—´
    - â€» ë¬¸ì„œ ì—†ìŒ ì²˜ë¦¬ ë¡œì§ì€ mainì—ì„œ ë‹´ë‹¹(LLM ë¯¸í˜¸ì¶œ)
    """
    lines = []
    lines.append("ì•„ë˜ 'ìë£Œ'ë§Œ ê·¼ê±°ë¡œ í•œêµ­ì–´ë¡œ ê°„ê²°íˆ ë‹µí•˜ì„¸ìš”.")
    lines.append("- ìë£Œ ë°– ì •ë³´ë¥¼ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.")
    lines.append("- ë‹µí•  ìˆ˜ ì—†ìœ¼ë©´ 'ì œê³µëœ ë¬¸ì„œì—ì„œ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.'ë¼ê³  ë§í•˜ì„¸ìš”.\n")
    lines.append(f"ì§ˆë¬¸:\n{query}\n")
    lines.append("ìë£Œ:")
    for i, d in enumerate(docs, 1):
        m = d.metadata
        lines.append(
            f"[ë¬¸ì„œ{i}] (source={m.get('source')}, ê¸°ì—…ëª…={m.get('ê¸°ì—…ëª…')}, ë‚ ì§œ={m.get('ë‚ ì§œ')}, "
            f"ì¹´í…Œê³ ë¦¬={m.get('ë¬¸ì„œ_ì¹´í…Œê³ ë¦¬')}, ì´ë²¤íŠ¸={m.get('ì£¼ìš”_ì´ë²¤íŠ¸')})\n{d.page_content}\n"
        )
    lines.append("ë‹µë³€:")
    return "\n".join(lines)

# 8) LLM í˜¸ì¶œ(ìƒì„±) --------------------------------------------------------------
def generate_with_llm(pipe, prompt: str):
    """
    - max_new_tokens / temperature / top_p ë“± í•˜ì´í¼íŒŒë¼ë¯¸í„°ëŠ” í•„ìš”ì— ë”°ë¼ ì¡°ì •
    - ë°˜í™˜: ëª¨ë¸ì´ ìƒì„±í•œ ë‹µë³€(í”„ë¡¬í”„íŠ¸ ì œì™¸)
    """
    out = pipe(
        prompt,
        max_new_tokens=256,
        do_sample=True,
        temperature=0.5,
        top_p=0.9,
        pad_token_id=pipe.tokenizer.eos_token_id,  # ì¼ë¶€ ëª¨ë¸ì—ì„œ í•„ìš”
    )
    full = out[0]["generated_text"]
    return full.split("ë‹µë³€:", 1)[-1].strip() if "ë‹µë³€:" in full else full[len(prompt):].strip()

# 9) Streamlit UI êµ¬ì„± -----------------------------------------------------------
def main():
    st.set_page_config(page_title="ğŸ¤– íˆ¬ì ì–´ì‹œìŠ¤í„´íŠ¸ (RAG)", page_icon="ğŸ¤–", layout="centered")
    st.title("ğŸ¤– íˆ¬ì ì–´ì‹œìŠ¤í„´íŠ¸ (RAG)")

    # 9-1) DB ë¡œë“œ ë° Retriever / LLM ì¤€ë¹„ (ìºì‹œ)
    try:
        texts, metadatas = load_documents_from_sqlite(DB_PATH)
    except FileNotFoundError as e:
        st.error(str(e))
        st.stop()

    if "retriever" not in st.session_state:
        st.session_state.retriever = build_ensemble_retriever(texts, metadatas)
    if "pipe" not in st.session_state:
        st.session_state.pipe = load_local_pipeline("Qwen/Qwen2.5-1.5B-Instruct") # Qwen/Qwen2.5-1.5B-Instruct, TinyLlama/TinyLlama-1.1B-Chat-v1.0, 42dot/42dot_LLM-SFT-1.3B

    # 9-2) ì±„íŒ… ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # 9-3) ê¸°ì¡´ ëŒ€í™” ë Œë”ë§
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    # 9-4) ì…ë ¥ì°½
    user_input = st.chat_input("ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”.")
    if user_input:
        st.chat_message("user").markdown(user_input)
        st.session_state.messages.append({"role": "user", "content": user_input})

        # (a) ê²€ìƒ‰
        docs = search(user_input.strip(), st.session_state.retriever)

        # (b) ë¬¸ì„œê°€ ì—†ìœ¼ë©´ LLM í˜¸ì¶œì„ 'í•˜ì§€ ì•Šê³ ' ì¦‰ì‹œ ê³ ì • ë‹µë³€ ë°˜í™˜  â†â˜… í•µì‹¬ ë³€ê²½
        if not docs:
            answer = "ì œê³µëœ ë¬¸ì„œì—ì„œ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        else:
            # (c) ë¬¸ì„œê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ í”„ë¡¬í”„íŠ¸ ìƒì„± í›„ LLM í˜¸ì¶œ
            prompt = build_prompt(user_input.strip(), docs)
            answer = generate_with_llm(st.session_state.pipe, prompt)

        # (d) ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ ì¶œë ¥/ì €ì¥
        with st.chat_message("assistant"):
            st.markdown(answer)
        st.session_state.messages.append({"role": "assistant", "content": answer})

        # (ì„ íƒ) ê²€ìƒ‰ëœ ë¬¸ì„œ íŒ¨ë„
        with st.expander("ğŸ” ì‚¬ìš©í•œ ìë£Œ(ê²€ìƒ‰ ê²°ê³¼) ë³´ê¸°", expanded=False):
            if not docs:
                st.markdown("_ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ_")
            else:
                for i, d in enumerate(docs, 1):
                    m = d.metadata
                    st.markdown(
                        f"**[ë¬¸ì„œ{i}]** (source={m.get('source')}, ê¸°ì—…ëª…={m.get('ê¸°ì—…ëª…')}, ë‚ ì§œ={m.get('ë‚ ì§œ')}, "
                        f"ì¹´í…Œê³ ë¦¬={m.get('ë¬¸ì„œ_ì¹´í…Œê³ ë¦¬')}, ì´ë²¤íŠ¸={m.get('ì£¼ìš”_ì´ë²¤íŠ¸')})\n\n"
                        f"{d.page_content}"
                    )

if __name__ == "__main__":
    main()
